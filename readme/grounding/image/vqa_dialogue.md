# Image-based VQA, Visual Reasoning and Dialogue


## Visual Question Answering and Visual Reasoning
- [2015 ICCV] **VQA: Visual Question Answering**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/VQA%20-%20Visual%20Question%20Answering.bib), [[homepage]](https://visualqa.org/index.html).
- [2016 NIPS] **Hierarchical Question-Image Co-Attention for Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1606.00061), [[bibtex]](/Bibtex/Hierarchical%20Question-Image%20Co-Attention%20for%20Visual%20Question%20Answering.bib), sources: [[karunraju/VQA]](https://github.com/karunraju/VQA), [[jiasenlu/HieCoAttenVQA]](https://github.com/jiasenlu/HieCoAttenVQA).
- [2016 CVPR] **Stacked Attention Networks for Image Question Answering**, [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf), [[bibtex]](/Bibtex/Stacked%20Attention%20Networks%20for%20Image%20Question%20Answering.bib), sources: [[zcyang/imageqa-san]](https://github.com/zcyang/imageqa-san).
- [2016 CVPR] **Neural Module Networks**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Andreas_Neural_Module_Networks_CVPR_2016_paper.pdf), [[bibtex]](/Bibtex/Neural%20Module%20Networks.bib).
- [2016 EMNLP] **Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**, [[paper]](https://www.aclweb.org/anthology/D16-1044.pdf), [[bibtex]](https://www.aclweb.org/anthology/D16-1044.bib), sources: [[akirafukui/vqa-mcb]](https://github.com/akirafukui/vqa-mcb), [[Cadene/vqa.pytorch]](https://github.com/Cadene/vqa.pytorch), [[MarcBS/keras]](https://github.com/MarcBS/keras).
- [2017 CVPR] **Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering**, [[paper]](https://zpascal.net/cvpr2017/Goyal_Making_the_v_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Making%20the%20V%20in%20VQA%20Matter%20-%20Elevating%20the%20Role%20of%20Image%20Understanding%20in%20Visual%20Question%20Answering.bib), [[homepage]](https://visualqa.org/).
- [2018 CVPR] **Visual Grounding via Accumulated Attention**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Visual%20Grounding%20via%20Accumulated%20Attention.bib).
- [2018 CVPR] **Donâ€™t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering**, [[paper]](https://zpascal.net/cvpr2018/Agrawal_Dont_Just_Assume_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Dont%20Just%20Assume%20Look%20and%20Answer%20-%20Overcoming%20Priors%20for%20Visual%20Question%20Answering.bib), [[homepage]](https://www.cc.gatech.edu/~aagrawal307/vqa-cp/), sources: [[AishwaryaAgrawal/GVQA]](https://github.com/AishwaryaAgrawal/GVQA).
- [2018 CVPR] **Referring Relationships**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Referring%20Relationships.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/), sources: [[StanfordVL/ReferringRelationships]](https://github.com/StanfordVL/ReferringRelationships).
- [2018 CVPR] **Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Bottom-Up%20and%20Top-Down%20Attention%20for%20Image%20Captioning%20and%20Visual%20Question%20Answering.bib), sources: [[peteanderson80/bottom-up-attention]](https://github.com/peteanderson80/bottom-up-attention), [[hengyuan-hu/bottom-up-attention-vqa]](https://github.com/hengyuan-hu/bottom-up-attention-vqa), [[LeeDoYup/bottom-up-attention-tf]](https://github.com/LeeDoYup/bottom-up-attention-tf).
- [2018 NeurIPS] **Overcoming Language Priors in Visual Question Answering with Adversarial Regularization**, [[paper]](http://papers.nips.cc/paper/7427-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization.pdf), [[bibtex]](/Bibtex/Overcoming%20Language%20Priors%20in%20Visual%20Question%20Answering%20with%20Adversarial%20Regularization.bib).
- [2019 AAAI] **Dynamic Capsule Attention for Visual Question Answering**, [[paper]](/Documents/Papers/Dynamic%20Capsule%20Attention%20for%20Visual%20Question%20Answering.pdf), [[bibtex]](/Bibtex/Dynamic%20Capsule%20Attention%20for%20Visual%20Question%20Answering.bib).
- [2019 AAAI] **BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection**, [[paper]](https://arxiv.org/pdf/1902.00038.pdf), [[bibtex]](/Bibtex/Block.bib), sources: [[Cadene/block.bootstrap.pytorch]](https://github.com/Cadene/block.bootstrap.pytorch).
- [2019 ACL] **Multi-grained Attention with Object-level Grounding for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/P19-1349.pdf), [[bibtex]](/Bibtex/Multi-grained%20Attention%20with%20Object-level%20Grounding%20for%20Visual%20Question%20Answering.bib).
- [2019 ACL] **A Corpus for Reasoning About Natural Language Grounded in Photographs**, [[paper]](https://www.aclweb.org/anthology/P19-1644v2.pdf), [[bibtex]](/Bibtex/A%20Corpus%20for%20Reasoning%20About%20Natural%20Language%20Grounded%20in%20Photographs.bib), [[homepage]](http://lil.nlp.cornell.edu/nlvr/).
- [2019 EMNLP] **B2T2: Fusion of Detected Objects in Text for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/D19-1219.pdf), [[bibtex]](/Bibtex/Fusion%20of%20Detected%20Objects%20in%20Text%20for%20Visual%20Question%20Answering.bib), sources: [[google-research/language/language/question_answering/b2t2/]](https://github.com/google-research/language/tree/master/language/question_answering/b2t2).
- [2019 ICCV] **Multi-modality Latent Interaction Network for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gao_Multi-Modality_Latent_Interaction_Network_for_Visual_Question_Answering_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Multi-modality%20Latent%20Interaction%20Network%20for%20Visual%20Question%20Answering.bib).
- [2019 ICCV] **Dynamic Graph Attention for Referring Expression Comprehension**, [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Dynamic_Graph_Attention_for_Referring_Expression_Comprehension_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Dynamic%20Graph%20Attention%20for%20Referring%20Expression%20Comprehension.bib), sources: [[sibeiyang/sgmn]](https://github.com/sibeiyang/sgmn).
- [2019 SIGIR] **Quantifying and Alleviating the Language Prior Problem in Visual Question Answering**, [[paper]](https://arxiv.org/pdf/1905.04877.pdf), [[bibtex]](/Bibtex/Quantifying%20and%20Alleviating%20the%20Language%20Prior%20Problem%20in%20Visual%20Question%20Answering.bib), sources: [[guoyang9/vqa-prior]](https://github.com/guoyang9/vqa-prior).
- [2019 CVPR] **Towards VQA Models That Can Read**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Towards%20VQA%20Models%20That%20Can%20Read.bib), sources: [[facebookresearch/pythia]](https://github.com/facebookresearch/pythia).
- [2019 CVPR] **From Recognition to Cognition: Visual Commonsense Reasoning**, [[paper]](https://arxiv.org/pdf/1811.10830.pdf), [[bibtex]](/Bibtex/From%20Recognition%20to%20Cognition%20-%20Visual%20Commonsense%20Reasoning.bib), [[homepage]](https://visualcommonsense.com), [[leaderboard]](https://visualcommonsense.com/leaderboard/), [[dataset]](https://visualcommonsense.com/download/), sources: [[rowanz/r2c]](https://github.com/rowanz/r2c/).
- [2019 CVPR] **Learning to Compose Dynamic Tree Structures for Visual Contexts**, [[paper]](https://zpascal.net/cvpr2019/Tang_Learning_to_Compose_Dynamic_Tree_Structures_for_Visual_Contexts_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Learning%20to%20Compose%20Dynamic%20Tree%20Structures%20for%20Visual%20Contexts.bib), sources: [[KaihuaTang/VCTree-Scene-Graph-Generation]](https://github.com/KaihuaTang/VCTree-Scene-Graph-Generation).
- [2019 CVPR] **Explicit Bias Discovery in Visual Question Answering Models**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Manjunatha_Explicit_Bias_Discovery_in_Visual_Question_Answering_Models_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Explicit%20Bias%20Discovery%20in%20Visual%20Question%20Answering%20Models.bib).
- [2019 CVPR] **GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering**, [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/GQA.bib), [[homepage]](visualreasoning.net).
- [2019 NeurIPS] **RUBi: Reducing Unimodal Biases for Visual Question Answering**, [[paper]](http://papers.nips.cc/paper/8371-rubi-reducing-unimodal-biases-for-visual-question-answering.pdf), [[bibtex]](/Bibtex/RUBi%20-%20Reducing%20Unimodal%20Biases%20for%20Visual%20Question%20Answering.bib), sources: [[cdancette/rubi.bootstrap.pytorch]](https://github.com/cdancette/rubi.bootstrap.pytorch).
- [2019 NeurIPS] **TAB-VCR: Tags and Attributes based VCR Baselines**, [[paper]](https://papers.nips.cc/paper/9693-tab-vcr-tags-and-attributes-based-vcr-baselines.pdf), [[bibtex]](/Bibtex/TAB-VCR%20-%20Tags%20and%20Attributes%20based%20VCR%20Baselines.bib), [[slides]](https://deanplayerljx.github.io/tabvcr/neurips_2019_slides.pdf), [[homepage]](https://deanplayerljx.github.io/tabvcr/), sources: [[Deanplayerljx/tab-vcr]](https://github.com/Deanplayerljx/tab-vcr).
- [2020 AAAI] **ManyModalQA: Modality Disambiguation and QA over Diverse Inputs**, [[paper]](https://arxiv.org/pdf/2001.08034.pdf), [[bibtex]](/Bibtex/ManyModalQA.bib), sources: [[hannandarryl/ManyModalQA]](https://github.com/hannandarryl/ManyModalQA).
- [2020 ACL] **Multimodal Neural Graph Memory Networks for Visual Question Answering**, [[paper]](https://www.aclweb.org/anthology/2020.acl-main.643.pdf), [[bibtex]](/Bibtex/Multimodal%20Neural%20Graph%20Memory%20Networks%20for%20Visual%20Question%20Answering.bib).
- [2020 ArXiv] **Regularizing Attention Networks for Anomaly Detection in Visual Question Answering**, [[paper]](https://arxiv.org/pdf/2009.10054.pdf), [[bibtex]](/Bibtex/Regularizing%20Attention%20Networks%20for%20Anomaly%20Detection%20in%20Visual%20Question%20Answering.bib), sources: [[LeeDoYup/Anomaly_Detection_VQA]](https://github.com/LeeDoYup/Anomaly_Detection_VQA).
- [2021 WACV] **Meta Module Network for Compositional Visual Reasoning**, [[paper]](https://arxiv.org/pdf/1910.03230.pdf), [[bibtex]](/Bibtex/Meta%20Module%20Network%20for%20Compositional%20Visual%20Reasoning.bib), sources: [[wenhuchen/Meta-Module-Network]](https://github.com/wenhuchen/Meta-Module-Network).

## Image-based Visual Dialogue
- [2019 ArXiv] **Two Causal Principles for Improving Visual Dialog: Visual Dialog Challenge 2019 Winner Report**, [[paper]](https://arxiv.org/pdf/1911.10496.pdf), [[bibtex]](/Bibtex/Two%20Causal%20Principles%20for%20Improving%20Visual%20Dialog%20-%20Visual%20Dialog%20Challenge%202019%20Winner%20Report.bib).