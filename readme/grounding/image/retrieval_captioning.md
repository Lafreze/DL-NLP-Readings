# Image Retrieval, Captioning and Edit

## Image based Retrieval
- [2019 CVPR] **Deep Supervised Cross-modal Retrieval**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhen_Deep_Supervised_Cross-Modal_Retrieval_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Deep%20Supervised%20Cross-modal%20Retrieval.bib).
- [2019 SIGIR] **Scalable Deep Multimodal Learning for Cross-Modal Retrieval**, [[paper]](/Documents/Papers/Scalable%20Deep%20Multimodal%20Learning%20for%20Cross-Modal%20Retrieval.pdf), [[bibtex]](/Bibtex/Scalable%20Deep%20Multimodal%20Learning%20for%20Cross-Modal%20Retrieval.bib).
- [2019 ACMMM] **A New Benchmark and Approach for Fine-grained Cross-media Retrieval**, [[paper]](https://arxiv.org/pdf/1907.04476.pdf), [[bibtex]](/Bibtex/A%20New%20Benchmark%20and%20Approach%20for%20Fine-grained%20Cross-media%20Retrieval.bib), [[homepage]](http://59.108.48.34/tiki/FGCrossNet/), sources: [[PKU-ICST-MIPL/FGCrossNet_ACMMM2019]](https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019).
- [2019 ICCV] **Adversarial Representation Learning for Text-to-Image Matching**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Sarafianos_Adversarial_Representation_Learning_for_Text-to-Image_Matching_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Adversarial%20Representation%20Learning%20for%20Text-to-Image%20Matching.bib).

## Image-based Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2015 NeurIPS] **Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks**, [[paper]](https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf), [[bibtex]](/Bibtex/Scheduled%20Sampling%20for%20Sequence%20Prediction%20with%20Recurrent%20Neural%20Networks.bib).
- [2016 NeurIPS] **Professor Forcing: A New Algorithm for Training Recurrent Networks**, [[paper]](http://papers.nips.cc/paper/6099-professor-forcing-a-new-algorithm-for-training-recurrent-networks.pdf), [[bibtex]](/Bibtex/Professor%20Forcing.bib), sources: [[anirudh9119/LM_GANS]](https://github.com/anirudh9119/LM_GANS).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2017 CVPR] **SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/SCA-CNN%20-%20Spatial%20and%20Channel-wise%20Attention%20in%20Convolutional%20Networks%20for%20Image%20Captioning.bib), sources: [[zjuchenlong/sca-cnn.cvpr17]](https://github.com/zjuchenlong/sca-cnn.cvpr17).
- [2017 CVPR] **Self-critical Sequence Training for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Self-critical%20Sequence%20Training%20for%20Image%20Captioning.bib), sources: [[ruotianluo/self-critical.pytorch]](https://github.com/ruotianluo/self-critical.pytorch).
- [2018 CVPR] **Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Bottom-Up%20and%20Top-Down%20Attention%20for%20Image%20Captioning%20and%20Visual%20Question%20Answering.bib), sources: [[peteanderson80/bottom-up-attention]](https://github.com/peteanderson80/bottom-up-attention), [[hengyuan-hu/bottom-up-attention-vqa]](https://github.com/hengyuan-hu/bottom-up-attention-vqa), [[LeeDoYup/bottom-up-attention-tf]](https://github.com/LeeDoYup/bottom-up-attention-tf).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).
- [2018 NeurIPS] **Partially-Supervised Image Captioning**, [[paper]](https://papers.nips.cc/paper/7458-partially-supervised-image-captioning.pdf), [[bibtex]](/Bibtex/Partially-Supervised%20Image%20Captioning.bib).
- [2019 CVPR] **Auto-Encoding Scene Graphs for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yang_Auto-Encoding_Scene_Graphs_for_Image_Captioning_CVPR_2019_paper.pdf), [[bibtex]](/Bibtex/Auto-Encoding%20Scene%20Graphs%20for%20Image%20Captioning.bib), [[post]](https://zhuanlan.zhihu.com/p/41200392).
- [2019 ICCV] **Attention on Attention for Image Captioning**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Attention_on_Attention_for_Image_Captioning_ICCV_2019_paper.pdf), [[bibtex]](/Bibtex/Attention%20on%20Attention%20for%20Image%20Captioning.bib), sources: [[husthuaan/AoANet]](https://github.com/husthuaan/AoANet).
- [2020 ArXiv] **Deconfounded Image Captioning: A Causal Retrospect**, [[paper]](https://arxiv.org/pdf/2003.03923.pdf), [[bibtex]](/Bibtex/Deconfounded%20image%20captioning.bib).

## Text-based Image Edit
- [2019 ACL] **Expressing Visual Relationships via Language**, [[paper]](https://www.aclweb.org/anthology/P19-1182.pdf), [[bibtex]](/Bibtex/Expressing%20Visual%20Relationships%20via%20Language.bib), sources: [[airsplay/VisualRelationships]](https://github.com/airsplay/VisualRelationships).